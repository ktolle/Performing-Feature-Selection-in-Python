{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Feature Selection\n",
    "Question: Why do we do feature selection? \n",
    "Answer: In order to have the most predictive model we can for the least computational cost. \n",
    "\n",
    "How we do this is by eliminating independent variables that are nonpredictive or only marginally so. This reduces the chance of overfitting to the features, increases accuracy and shortens time to convergence. \n",
    "\n",
    "This notebook is a walk through several of the examples in the scikit learn site(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), back-to-back. I not only show you how to find out the most predictive features, I show you how to display them to the screen and put these top features into a new dataframe so that you can use that dataframe as input to a downstream process (something often frustratingly not shown my others). \n",
    "\n",
    "Caveat: I should have written the conversion to a new dataframe as a #def, but I got too focused on finishing it. On the one hand, that means you can just use each section as a \"complete\" notebook. I often do this because it is easier in the classroom to show them inline. There is also some slight variations due to different attributes that are available for each feature selection method. The only downside is that this notebook is much longer than most of the ones I publish.\n",
    "\n",
    "The Feature Selection Techniques covered are: \n",
    "* SelectKBest\n",
    "* Recursive Feature Elimination (RFE)\n",
    "* RFE with Cross Validation (a favorite of mine as my students know)\n",
    "* SelectFromModel\n",
    "    * Lasso\n",
    "    * Random Forest Classifier\n",
    "* Extra Tree Classification\n",
    "\n",
    "NOTE: these are being used for classification and the dataset is the extended Wisconsin Breast Cancer dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in the file from UCI <recommend you save locally and load it if your connectivity is iffy>\n",
    "\n",
    "# Loading the file over the internet\n",
    "filename = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\" \n",
    "\n",
    "# Loading the file locally in the same folder as the Python Notebook\n",
    "#filename = \"wi_breast_cancer.csv\"\n",
    "names = ['ID','Diagnosis',\n",
    "         'Mean-Radius','Mean-Texture','Mean-Perimeter',\n",
    "         'Mean-Area','Mean-Smoothness','Mean-Compactness',\n",
    "         'Mean-Concavity','Mean-ConcavePoints',\n",
    "         'Mean-Symmetry','Mean-FractalDimension', \n",
    "         'StdErr-Radius','StdErr-Texture','StdErr-Perimeter',\n",
    "         'StdErr-Area','StdErr-Smoothness','StdErr-Compactness',\n",
    "         'StdErr-Concavity','StdErr-ConcavePoints',\n",
    "         'StdErr-Symmetry','StdErr-FractalDimension',\n",
    "         'Worst-Radius','Worst-Texture','Worst-Perimeter',\n",
    "         'Worst-Area','Worst-Smoothness','Worst-Compactness',\n",
    "         'Worst-Concavity','Worst-ConcavePoints',\n",
    "         'Worst-Symmetry','Worst-FractalDimension']\n",
    "\n",
    "# loading the file into a dataframe\n",
    "data = pd.read_csv(filename, names=names, header=None) \n",
    "\n",
    "# Convert the Diagnosis to a numeric variable\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Malignant tumors = 1 or True and Benign tumors = 0 or False\n",
    "\n",
    "# Loading the X and y matrices\n",
    "X = data.iloc[:, 2:32]   # load features into X dataframe\n",
    "Y = data.iloc[:, 1]      # Load target into y dataframe\n",
    "\n",
    "# Get the rows and columns of the numpy array\n",
    "(nRows, nCols) = X.shape\n",
    "X.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest Features \n",
    "Testing SelectKBest in order to ensure we are using the right features for our dataset. The example below uses the Chi-Squared ${(Ï‡2)}$ statistical test for non-negative features to select the best features from the dataset. The method it uses for selecting them is a one-way ANOVA F-test. \n",
    "\n",
    "A large score suggests that the means of the that ${K}$ groups are not all equal. This is true only when the input variables come from normally distributed populations, and the population variance of the ${K}$ are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Setting precision for display\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "fitScores = []\n",
    "\n",
    "# feature extraction; where k is the number of features you want to select\n",
    "test = SelectKBest(score_func=chi2, k=5)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Find the scores for every feature so that you know which were selected\n",
    "fitScores = fit.scores_\n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "features = pd.DataFrame(fitScores.reshape(-1, len(fitScores)),columns=names[2:32])\n",
    "print(features.T) # transpose to make it easier to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In eyeballing the data we can see the variables with the highest scores are (in order of score): Worst-Area, Mean-Area, StdErr-Area, Worst-Perimeter and Mean-Perimeter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand coding the headers, but will show later how to do this automatically\n",
    "colHeads = ['Mean-Perimeter','Mean-Area','StdErr-Area','Worst-Perimeter','Worst-Area']\n",
    "\n",
    "# perform the selection of fields so we have them for later analysis\n",
    "kSelect = SelectKBest(chi2, k=5).fit_transform(X, Y)\n",
    "(rows, cols) = kSelect.shape \n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selected = pd.DataFrame(data=kSelect,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selected.columns = colHeads \n",
    "selected.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the p-values returned to see what light this might shed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitPValues = fit.pvalues_\n",
    "# print(type(fitPValues)) \n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "pValues = pd.DataFrame(fitPValues.reshape(-1, len(fitScores)),columns=names[2:32]).T\n",
    "print(pValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we find is that MANY of these would be consider significant. So perhaps we should be looking, not at a number to keep, but using this technique to eliminate features. But if there is a massive skew to the data, we might be eliminating good variables, using this method, simply because they are normally distributed. \n",
    "\n",
    "What this tells us is that it is likely that the following should be considered for removal because they have a p-value above 0.5 (not to be confused with 0.05): \n",
    "* Mean-Smoothness\n",
    "* Mean-Symmetry\n",
    "* Mean-FractalDimension*\n",
    "* StdErr-Texture*\n",
    "* StdErr-Smoothness*\n",
    "* StdErr-Compactness\n",
    "* StdErr-ConcavePoints\n",
    "* StdErr-Symmetry*\n",
    "* StdErr-FractalDimension*\n",
    "* Worst-Smoothness\n",
    "* Worst-FractalDimension\n",
    "\n",
    "This would *remove* the 11 least predictive. If we choose to be even more careful and use 0.9 or higher, we'd drop 5 features marked with astericks above. \n",
    "\n",
    "Let's see if any of these \"bad\" variables is selected using other techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Recursively removes attributes and builds models on those attributes that remain. It accomplishes this by training on the full set then determining the feature importances given the model selected then it prunes the worst, the next worst and so on building a model each time until it ends up with the final set. Default removal each time (step) is one.\n",
    "\n",
    "Let's see what the 10 most predictive features are with RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "numFeat = 8 # change here to set number of features to \"keep\"\n",
    "\n",
    "# feature extraction \n",
    "model = LogisticRegression() \n",
    "rfe = RFE(model, numFeat) # where the number is the features retained\n",
    "rfe = rfe.fit(X,Y) \n",
    "\n",
    "ranking = rfe.ranking_\n",
    "selected = rfe.support_\n",
    "\n",
    "ranking = np.vstack((ranking, selected))\n",
    "\n",
    "(rows, cols) = ranking.shape\n",
    "\n",
    "# This dataframe doesn't hold the columns selected, \n",
    "# it is only for pretty printing the selected features\n",
    "rfe_selected = pd.DataFrame(data=ranking,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "rfe_selected.columns = names[2:32] \n",
    "\n",
    "array = rfe_selected.T # transpose\n",
    "array.columns = ['rank', 'selected']\n",
    "output = array['selected'] == 1\n",
    "df = array[selected]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SelectKBest said the following are the \"worst\":\n",
    "* Mean-FractalDimension\n",
    "* StdErr-Texture\n",
    "* StdErr-Smoothness\n",
    "* StdErr-Symmetry\n",
    "* StdErr-FractalDimension\n",
    "\n",
    "None of these were chosen to be kept--so I feel pretty confident in dropping them. In fact, none of the ones with a p-value above 0.5 made the cut. So I *might* feel okay about dropping all 11 by now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual features selected for later processing\n",
    "rfeSelect = RFE(model,numFeat).fit_transform(X, Y)\n",
    "\n",
    "# Get the size of the array of selected values \n",
    "(rows, cols) = rfeSelect.shape\n",
    "\n",
    "# Get the column headings and remove the selection data\n",
    "df2 = df.T # transpose back... :-)\n",
    "heads = df2.iloc[0:0]\n",
    "heads = heads.columns\n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selectedRFE = pd.DataFrame(data=rfeSelect,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selectedRFE.columns = heads \n",
    "selectedRFE.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we aligned on ones to drop, what about the ones to keep? SelectKBest chose: Mean-Perimeter, Mean-Area, StdErr-Area, Worst-Perimeter, and Worst-Area. In RFE, only two of these \"keepers\" made the list: StdErr-Area, Worst-Perimeter. \n",
    "\n",
    "The most important thing to note is that the top ones do not correspond with kSelect choices. Given this is a more robust method of feature selection, and that it was tested by removing variables to see which models perform best, I think we can safely say, that variables selected with RFE are more likely to generate a more accurate model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive removal with cross validation\n",
    "In this case we will be using a support vector machine and RFECV to identify the top features. This is still a recursive removal. The difference is that while RFE is stronger than KSelectBest, this method is stronger still and has the benefit of not having to set the number of best features in advance. Instead, it automatically determines of the number of features to be selected, rather than making the data scientist find the best number of features to keep by trial and error. \n",
    "\n",
    "So what is the best number to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\") # using linear, but also use poly or radial basis \n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, Y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "#print(\"Selected Features: %s\" % rfecv.support_) \n",
    "#print(\"Feature Ranking: %s\" % rfecv.ranking_)\n",
    "\n",
    "rankingCV = rfecv.ranking_\n",
    "selectedCV = rfecv.support_\n",
    "\n",
    "rankingCV = np.vstack((rankingCV, selectedCV))\n",
    "(rows, cols) = rankingCV.shape\n",
    "\n",
    "# This dataframe for pretty printing the selected features\n",
    "rfecv_selected = pd.DataFrame(data=rankingCV,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "rfecv_selected.columns = names[2:32] \n",
    "\n",
    "arrayCV = rfecv_selected.T\n",
    "arrayCV.columns = ['rankCV', 'selectedCV']\n",
    "output = arrayCV['selectedCV'] == 1\n",
    "dfCV = arrayCV[selectedCV]\n",
    "print(dfCV)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "# it's handy that RFECV has the grid_scores features\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual features selected for later processing\n",
    "rfecvFeatures = rfecv.transform(X)\n",
    "\n",
    "# Get the size of the array of selected values \n",
    "(rows, cols) = rfecvFeatures.shape\n",
    "#print(rows, cols)\n",
    "\n",
    "# Get the column headings and remove the selection data\n",
    "df3 = dfCV.T\n",
    "heads = df3.iloc[0:0]\n",
    "heads = heads.columns\n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selectedRFECV = pd.DataFrame(data=rfecvFeatures,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selectedRFECV.columns = heads \n",
    "selectedRFECV.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RFE selected the following 10: Mean-Radius, Mean-Concavity, Mean-ConcavePoints, StdErr-Perimeter, StdErr-Area*, Worst-Radius, Worst-Perimeter*, Worst-Compactness, Worst-Concavity, and Worst-ConcavePoints. Two of these, StdErr-Area and Worst-Perimeter were *not* chosen. \n",
    "\n",
    "We can also see on the plot that there is significant variance explained by five features (see the local maxima at 4-5). These generally correspnd with RFE's first five. But it is the next 3 features, when included with the first five, that result in even better predictability. We plateau again after that, but don't see a drop in predictability until we reach 19 variables, where the prediction actually gets worse by including these and the remainder variables. Most of these come from those that were generated using standard error. \n",
    "\n",
    "So do the eight correspond between the two methods? We'd have to compare the *ranking_* values to be sure. I'll leave that to you to try. Go back to RFE, choose 8 variables and look at the ranked list. *And no, I haven't done it so I can't 'just tell you'.* Okay, I have, but then you wouldn't do it. ;)\n",
    "\n",
    "In the meantime, I would simply say that if you have the computational cycles for this approach, this method is the best so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select From Model\n",
    "This is what is referred to as a \"meta-transformer\" which is a model-based approach. This type of approach uses machine learning to model the data, judging the usefulness of a feature according to its relative importance to the predictability of the target variable. \n",
    "\n",
    "It can be used alongside any type of estimator with the coeffient (coef) or feature importance attribute post fitting the data to the model. Instead of selecting the number of features, it selects features that are below a threshold that you provide. The trick is knowing what that threshold should be, but there are ways, as we saw in RFECV to get at this information. In this instance, I'll be using the LASSO cross validation technique (Lasso.CV) which uses KFold as the cross validator by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "clf = LassoCV(cv=5)\n",
    "sfm = SelectFromModel(clf) \n",
    "# default threshold for Lasso is 1e-5\n",
    "\n",
    "#     features to align with previous methods\n",
    "sfmFeatures = sfm.fit_transform(X,Y)\n",
    "\n",
    "(rows, cols) = sfmFeatures.shape\n",
    "print(rows, cols)\n",
    "\n",
    "# This dataframe is for pretty printing the selected features\n",
    "sfm_selected = pd.DataFrame(data=sfmFeatures,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "sfm_selected.columns = ['Mean-Area','Worst-Texture','Worst-Perimeter',\"Worst-Area\"] \n",
    "\n",
    "sfm_selected.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I had to \"eyeball\" this one (go back to the datafile) to find the features that were selected due to the lack of \"helper\" functions I used in the previous sections. What might jump out at you is that Worst-Perimeter and Worst-Area, dropped by the RFECV, were both selected with this method. I scrubbed through documentation, looked at Stackoverflow--no explanation, poor documentation. And for code that hasn't been updated since 2014... \n",
    "\n",
    "Let's swap out Lasso with RandowForest and see how that might change things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: Decision Trees in SciKit Learn, as they are implemented, are stochastic. The issue is that each time you run it you will get a different set of features--which sucks if you are looking for a consistent set to choose from and expect the same results every time--AND DON'T GET THEM. \n",
    "\n",
    "The trick, learned on StackOverflow, is to add the line of code below to make it deterministic.\n",
    "\n",
    "The other problem, as with some examples above is the lack of helper classes, but at least with trees you can visualize them. See my Decision Tree lesson for how you can do that https://github.com/ktolle/DecisionTrees/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101) # make this stochastic decision tree deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sfmRFC = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=7))\n",
    "\n",
    "#     features to align with previous methods\n",
    "sfmRFC_Features = sfmRFC.fit_transform(X,Y)\n",
    "\n",
    "(rows, cols) = sfmRFC_Features.shape\n",
    "print(rows, cols)\n",
    "\n",
    "# This dataframe is for pretty printing the selected features\n",
    "sfmRFC_selected = pd.DataFrame(data=sfmRFC_Features,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "sfmRFC_selected.columns = ['Mean-Radius','Mean-Perimeter','Mean-Area',\n",
    "                        'Mean-Concavity','Mean-ConcavePoints','Worst-Radius',\n",
    "                        'Worst-Perimeter','Worst-Area',\n",
    "                        'Worst-Concavity','Worst-ConcavePoints']\n",
    "\n",
    "sfmRFC_selected.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a \"greedy\" approach to variable selection as so it might overselect variables which are less useful. Note that Worst-Texture was dropped from the list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance via ExtraTreesClassifier\n",
    "Bagged trees like Random Forest and Extra Trees can be used to estimate the importance of features. Extra Trees implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier().fit(X, Y) \n",
    "etcFeatures = etc.feature_importances_\n",
    "\n",
    "dfFeatures = pd.DataFrame(etcFeatures.reshape(-1, len(etcFeatures)),columns=names[2:32])\n",
    "dfFeatures.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach tells us that the most important features **in order** are: 1st: Worst-Area, 2nd: StdErr-Radius, 3rd: Mean-ConcavePoints, 4-5th: Worst-Perimeter and Mean-Radius, 6th: Mean-Area, 7th: Mean-Perimeter, 8th: Worst-ConcavePoints, 9-10th: Worst-Radius and StdErr-Area. Or in variable order:\n",
    "\n",
    "* Mean-Radius\n",
    "* Mean-Perimeter\n",
    "* Mean-Area\n",
    "* Mean-Concavity\n",
    "* StdErr-Radius*\n",
    "* StdErr-Area*\n",
    "* Worst-Radius\n",
    "* Worst-Perimeter\n",
    "* Worst-Area\n",
    "* Worst-ConcavePoints\n",
    "\n",
    "So the results are different again. The two are marked with astericks above. However they are not wildly different across approaches. So we've really narrowed in on the best and worst. \n",
    "\n",
    "It's worth noting that the \"maxima\" we hit earlier was at the 4-5 variable range. It's also worth noting we have not normalized/standardized the data. Why or why not might this be a problem? Try that next. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
