{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Feature Selection\n",
    "Question: Why do we do feature selection? \n",
    "Answer: In order to have the most predictive model we can for the least computational cost. \n",
    "\n",
    "How we do this is by eliminating independent variables that are nonpredictive or only marginally so. This reduces the chance of overfitting to the features, increases accuracy and shortens time to convergence. \n",
    "\n",
    "This notebook is a walk through several of the examples in the scikit learn site(https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection), back-to-back. I not only show you how to find out the most predictive features, I show you how to display them to the screen and put these top features into a new dataframe so that you can use that dataframe as input to a downstream process (something often frustratingly not shown my others). \n",
    "\n",
    "Caveat: I should have written the conversion to a new dataframe as a #def, but I got too focused on finishing it. On the one hand, that means you can just use each section as a \"complete\" notebook. I often do this because it is easier in the classroom to show them inline. There is also some slight variations due to different attributes that are available for each feature selection method. The only downside is that this notebook is much longer than most of the ones I publish.\n",
    "\n",
    "The Feature Selection Techniques covered are: \n",
    "* SelectKBest\n",
    "* Recursive Feature Elimination (RFE)\n",
    "* RFE with Cross Validation (a favorite of mine as my students know)\n",
    "* SelectFromModel\n",
    "    * Lasso\n",
    "    * Random Forest Classifier\n",
    "* Extra Tree Classification\n",
    "\n",
    "NOTE: these are being used for classification and the dataset is the extended Wisconsin Breast Cancer dataset: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in the file from UCI <recommend you save locally and load it if your connectivity is iffy>\n",
    "\n",
    "# Loading the file over the internet\n",
    "filename = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\" \n",
    "\n",
    "# Loading the file locally in the same folder as the Python Notebook\n",
    "#filename = \"wi_breast_cancer.csv\"\n",
    "names = ['ID','Diagnosis',\n",
    "         'Mean-Radius','Mean-Texture','Mean-Perimeter',\n",
    "         'Mean-Area','Mean-Smoothness','Mean-Compactness',\n",
    "         'Mean-Concavity','Mean-ConcavePoints',\n",
    "         'Mean-Symmetry','Mean-FractalDimension', \n",
    "         'StdErr-Radius','StdErr-Texture','StdErr-Perimeter',\n",
    "         'StdErr-Area','StdErr-Smoothness','StdErr-Compactness',\n",
    "         'StdErr-Concavity','StdErr-ConcavePoints',\n",
    "         'StdErr-Symmetry','StdErr-FractalDimension',\n",
    "         'Worst-Radius','Worst-Texture','Worst-Perimeter',\n",
    "         'Worst-Area','Worst-Smoothness','Worst-Compactness',\n",
    "         'Worst-Concavity','Worst-ConcavePoints',\n",
    "         'Worst-Symmetry','Worst-FractalDimension']\n",
    "\n",
    "# loading the file into a dataframe\n",
    "data = pd.read_csv(filename, names=names, header=None) \n",
    "\n",
    "# Convert the Diagnosis to a numeric variable\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Malignant tumors = 1 or True and Benign tumors = 0 or False\n",
    "\n",
    "# Loading the X and y matrices\n",
    "X = data.iloc[:, 2:32]   # load features into X dataframe\n",
    "Y = data.iloc[:, 1]      # Load target into y dataframe\n",
    "\n",
    "# Get the rows and columns of the numpy array\n",
    "(nRows, nCols) = X.shape\n",
    "#X.head(0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SelectKBest Features \n",
    "Testing SelectKBest in order to ensure we are using the right features for our dataset. The example below uses the Chi-Squared ${(Ï‡2)}$ statistical test for non-negative features to select the best features from the dataset. The method it uses for selecting them is a one-way ANOVA F-test. \n",
    "\n",
    "A large score suggests that the means of the that ${K}$ groups are not all equal. This is true only when the input variables come from normally distributed populations, and the population variance of the ${K}$ are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# Setting precision for display\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "np.set_printoptions(precision = 2)\n",
    "\n",
    "fitScores1 = []\n",
    "\n",
    "# feature extraction; where k is the number of features you want to select\n",
    "test = SelectKBest(score_func=chi2, k=5)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Find the scores for every feature so that you know which were selected\n",
    "fitScores1 = fit.scores_\n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "features1 = pd.DataFrame(fitScores1.reshape(-1, len(fitScores1)),columns=names[2:32]).T\n",
    "print(features1) # transpose to make it easier to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In eyeballing the data we can see the variables with the highest scores are (in order of score): Worst-Area, Mean-Area, StdErr-Area, Worst-Perimeter and Mean-Perimeter. \n",
    "\n",
    "Let's create a dataframe of just the selected variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand coding the headers, but will show later how to do this automatically\n",
    "colHeads = ['Mean-Perimeter','Mean-Area','StdErr-Area','Worst-Perimeter','Worst-Area']\n",
    "\n",
    "# perform the selection of fields so we have them for later analysis\n",
    "kSelect = SelectKBest(chi2, k=5).fit_transform(X, Y)\n",
    "(rows, cols) = kSelect.shape \n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selected = pd.DataFrame(data=kSelect,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selected.columns = colHeads \n",
    "selected.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the p-values returned to see what light this might shed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitPValues1 = fit.pvalues_\n",
    "# print(type(fitPValues)) \n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "pValues1 = pd.DataFrame(fitPValues1.reshape(-1, len(fitScores1)),columns=names[2:32]).T\n",
    "print(pValues1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we find is that MANY of these would be consider significant. So perhaps we should be looking, not at a number to keep, but using this technique to eliminate features. But if there is a massive skew to the data, we might be eliminating good variables, using this method, simply because they are normally distributed. \n",
    "\n",
    "What this tells us is that it is likely that the following should be considered for removal because they have a p-value above 0.5 (not to be confused with 0.05): \n",
    "* Mean-Smoothness\n",
    "* Mean-Symmetry\n",
    "* Mean-FractalDimension*\n",
    "* StdErr-Texture*\n",
    "* StdErr-Smoothness*\n",
    "* StdErr-Compactness\n",
    "* StdErr-ConcavePoints\n",
    "* StdErr-Symmetry*\n",
    "* StdErr-FractalDimension*\n",
    "* Worst-Smoothness\n",
    "* Worst-FractalDimension\n",
    "\n",
    "This would *remove* the 11 least predictive. If we choose to be even more careful and use 0.9 or higher, we'd drop 5 features marked with astericks above. \n",
    "\n",
    "Let's try scaling the data and see what we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X) \n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "(rows, cols) = rescaledX.shape\n",
    "\n",
    "scaleX = pd.DataFrame(data=rescaledX,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "scaleX.columns = names[2:32] \n",
    "\n",
    "# initalize new arrays\n",
    "fitScores2 = []\n",
    "fitpValues2 = []\n",
    "\n",
    "# feature extraction; where k is the number of features you want to select\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(scaleX, Y)\n",
    "\n",
    "# Find the scores for every feature so that you know which were selected\n",
    "fitScores2 = fit.scores_\n",
    "fitPValues2 = fit.pvalues_\n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "features2 = pd.DataFrame(fitScores2.reshape(-1, len(fitScores2)),columns=names[2:32]).T\n",
    "#print(features.T) # transpose to make it easier to read vertically\n",
    "\n",
    "# Convert the numpy array of scores back into a DF with the correct column names\n",
    "pValues2 = pd.DataFrame(fitPValues2.reshape(-1, len(fitScores2)),columns=names[2:32]).T\n",
    "#print(pValues.T)\n",
    "\n",
    "# Merge two dataframes into one\n",
    "df1 = pd.concat([features1, pValues1], axis=1, names = heads1)\n",
    "df2 = pd.concat([features2, pValues2], axis=1, names = heads2)\n",
    "frame = pd.concat([df1, df2], axis=1)\n",
    "frame.columns = ['Features1', 'pValues1', 'Features2', 'pValues2']\n",
    "frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling has an impact. Effectively all the p-values that are 0.00 are likely to be significant predictors and they vary when the dataset is scaled. We would keep more of them. \n",
    "\n",
    "Let's see if any of these \"bad\" variables is selected using other techniques. \n",
    "\n",
    "NOTE: X is unscaled at this point. If you want to run the rest of the notebook as scaled values run the reassigned below\n",
    "\n",
    "    X = scaleX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaleX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "Recursively removes attributes and builds models on those attributes that remain. It accomplishes this by training on the full set then determining the feature importances given the model selected then it prunes the worst, the next worst and so on building a model each time until it ends up with the final set. Default removal each time (step) is one.\n",
    "\n",
    "Let's see what the 10 most predictive features are with RFE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "numFeat = 10 # change here to set number of features to \"keep\"\n",
    "\n",
    "# feature extraction \n",
    "model = LogisticRegression() \n",
    "rfe = RFE(model, numFeat) # where the number is the features retained\n",
    "rfe = rfe.fit(X,Y) \n",
    "\n",
    "ranking = rfe.ranking_\n",
    "selected = rfe.support_\n",
    "\n",
    "ranking = np.vstack((ranking, selected))\n",
    "\n",
    "(rows, cols) = ranking.shape\n",
    "\n",
    "# This dataframe doesn't hold the columns selected, \n",
    "# it is only for pretty printing the selected features\n",
    "rfe_selected = pd.DataFrame(data=ranking,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "rfe_selected.columns = names[2:32] \n",
    "\n",
    "array = rfe_selected.T # transpose\n",
    "array.columns = ['rank', 'selected']\n",
    "output = array['selected'] == 1\n",
    "dfSelect = array[selected]\n",
    "dfSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all of the features and find the worst of the worst\n",
    "rankAll = pd.DataFrame(data=ranking,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "rankAll.columns = names[2:32] \n",
    "\n",
    "array = rankAll.T # transpose\n",
    "array.columns = ['Rank', 'Selected']\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I want now is to create a dataframe of those that were suggested to keep. This is just good to have code, but chances are I'm not done with feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual features selected for later processing\n",
    "rfeSelect = RFE(model,numFeat).fit_transform(X, Y)\n",
    "\n",
    "# Get the size of the array of selected values \n",
    "(rows, cols) = rfeSelect.shape\n",
    "\n",
    "# Get the column headings and remove the selection data\n",
    "df2 = df.T # transpose back... :-)\n",
    "heads = df2.iloc[0:0]\n",
    "heads = heads.columns\n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selectedRFE = pd.DataFrame(data=rfeSelect,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selectedRFE.columns = heads \n",
    "selectedRFE.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare across the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge two dataframes into one\n",
    "newFrame = pd.concat([frame, array], axis=1)\n",
    "newFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is inconsistency on feature selection (if you scaled your data), but it is fairly minor. What worries me more is the ones I'm leaving on the cutting room floor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive removal with cross validation\n",
    "In this case we will be using a support vector machine and RFECV to identify the top features. This is still a recursive removal. The difference is that while RFE is stronger than KSelectBest, this method is stronger still and has the benefit of not having to set the number of best features in advance. Instead, it automatically determines of the number of features to be selected, rather than making the data scientist find the best number of features to keep by trial and error. \n",
    "\n",
    "So what is the best number to keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "svc = SVC(kernel=\"linear\") # using linear, but also use poly or radial basis \n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(X, Y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "#print(\"Selected Features: %s\" % rfecv.support_) \n",
    "#print(\"Feature Ranking: %s\" % rfecv.ranking_)\n",
    "\n",
    "rankingCV = rfecv.ranking_\n",
    "selectedCV = rfecv.support_\n",
    "\n",
    "rankingCV = np.vstack((rankingCV, selectedCV))\n",
    "(rows, cols) = rankingCV.shape\n",
    "\n",
    "# This dataframe for pretty printing the selected features\n",
    "rfecv_selected = pd.DataFrame(data=rankingCV,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "rfecv_selected.columns = names[2:32] \n",
    "\n",
    "arrayCV = rfecv_selected.T\n",
    "arrayCV.columns = ['rankCV', 'selectedCV']\n",
    "output = arrayCV['selectedCV'] == 1\n",
    "dfCV = arrayCV[selectedCV]\n",
    "print(dfCV)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "# it's handy that RFECV has the grid_scores features\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on whether you scaled or not, you'll see that there is either 18 or 20 features kept. I feel validated in being concerned about selecting the top ten. And I like that the number is chosen for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the actual features selected for later processing\n",
    "rfecvFeatures = rfecv.transform(X)\n",
    "\n",
    "# Get the size of the array of selected values \n",
    "(rows, cols) = rfecvFeatures.shape\n",
    "#print(rows, cols)\n",
    "\n",
    "# Get the column headings and remove the selection data\n",
    "df3 = dfCV.T\n",
    "heads = df3.iloc[0:0]\n",
    "heads = heads.columns\n",
    "\n",
    "# Create a dataframe to hold the selected values (only) for later processing\n",
    "selectedRFECV = pd.DataFrame(data=rfecvFeatures,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "# Add the column headers for the X array--the range from names for this dataframe\n",
    "selectedRFECV.columns = heads \n",
    "selectedRFECV.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see on the plot that there is significant variance explained by 4-5 features (see the local maxima at 4-5). These generally correspnd with RFE's first five. But it is the next 3 features, when included with the first five, that result in even better predictability. We plateau again after that.\n",
    "\n",
    "If you have the computational cycles for this approach, this method is the best so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select From Model\n",
    "This is what is referred to as a \"meta-transformer\" which is a model-based approach. This type of approach uses machine learning to model the data, judging the usefulness of a feature according to its relative importance to the predictability of the target variable. \n",
    "\n",
    "It can be used alongside any type of estimator with the coeffient (coef) or feature importance attribute post fitting the data to the model. Instead of selecting the number of features, it selects features that are below a threshold that you provide. The trick is knowing what that threshold should be, but there are ways, as we saw in RFECV to get at this information. In this instance, I'll be using the LASSO cross validation technique (Lasso.CV) which uses KFold as the cross validator by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "clf = LassoCV(cv=5)\n",
    "sfm = SelectFromModel(clf) \n",
    "# default threshold for Lasso is 1e-5\n",
    "\n",
    "#     features to align with previous methods\n",
    "sfmFeatures = sfm.fit_transform(X,Y)\n",
    "\n",
    "(rows, cols) = sfmFeatures.shape\n",
    "print(rows, cols)\n",
    "\n",
    "# This dataframe is for pretty printing the selected features\n",
    "sfm_selected = pd.DataFrame(data=sfmFeatures,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "\n",
    "sfm_selected.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I scale I get a \"warning\" which does not appear when I don't. I was not able to \"label\" the column headings in this instance due to the lack of helper functions. For this method, I'd have to eyeball it, which is less than ideal. \n",
    "\n",
    "Let's swap out Lasso with RandowForest and see how that might change things. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTANT NOTE: Decision Trees in SciKit Learn, as they are implemented, are stochastic. The issue is that each time you run it you will get a different set of features--which sucks if you are looking for a consistent set to choose from and expect the same results every time and get different ones on each run. \n",
    "\n",
    "The trick, learned on StackOverflow, is to add the line of code below to make it deterministic.\n",
    "\n",
    "    np.random.seed(101)\n",
    "\n",
    "The other problem, as with some examples above is the lack of helper classes, but at least with trees you can visualize them. See my Decision Tree lesson for how you can do that https://github.com/ktolle/DecisionTrees/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(101) # make this stochastic decision tree deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sfmRFC = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=7))\n",
    "\n",
    "#     features to align with previous methods\n",
    "sfmRFC_Features = sfmRFC.fit_transform(X,Y)\n",
    "\n",
    "(rows, cols) = sfmRFC_Features.shape\n",
    "print(rows, cols)\n",
    "\n",
    "# This dataframe is for pretty printing the selected features\n",
    "sfmRFC_selected = pd.DataFrame(data=sfmRFC_Features,\n",
    "          index=np.array(range(1, rows+1)),\n",
    "          columns=np.array(range(1, cols+1)))\n",
    "sfmRFC_selected.columns = ['Mean-Radius','Mean-Perimeter','Mean-Area',\n",
    "                        'Mean-Concavity','Mean-ConcavePoints','Worst-Radius',\n",
    "                        'Worst-Perimeter','Worst-Area',\n",
    "                        'Worst-Concavity','Worst-ConcavePoints']\n",
    "#print(X.head(1).T)\n",
    "sfmRFC_selected.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is a \"greedy\" approach to variable selection as so it might overselect variables which are less useful. Note that Worst-Texture was dropped from the list. \n",
    "\n",
    "NOTE: unlike other methods, scaling had no impact on this method of variable selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier().fit(X, Y) \n",
    "etcFeatures = etc.feature_importances_\n",
    "\n",
    "dfFeatures = pd.DataFrame(etcFeatures.reshape(-1, len(etcFeatures)),columns=names[2:32])\n",
    "\n",
    "dfFeatures.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are different again AND they change if you scale your data. However the results aren't not wildly different across approaches. So we've really narrowed in on the best and worst. \n",
    "\n",
    "It's worth noting that the \"maxima\" we hit earlier was at the 4-5 variable range. It's also worth noting we have not normalized. Why or why not might this be a problem? Try that next. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
